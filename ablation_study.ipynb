{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/diff_lora/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x137cf6480>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from model import load_models, initialize_pipeline\n",
    "from utils import load_config\n",
    "from lora import apply_lora_replacement, disable_lora, enable_lora, load_all_lora_weights\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config()\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LORA_RANK = 128\n",
    "LORA_ALPHA = 64\n",
    "ABLATION_STUDY_DIR = os.path.join(config.get(\"RESULTS_PATH\"), \"ablation_study\")\n",
    "os.makedirs(ABLATION_STUDY_DIR, exist_ok=True)\n",
    "\n",
    "PROMPT = \"The wolf from the cartoon “Well, Wait!”\"\n",
    "SAVE_DIR = config.get(\"TRAINING_FOLDER_NAME\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n"
     ]
    }
   ],
   "source": [
    "vae, unet, text_encoder, tokenizer, noise_scheduler = load_models()\n",
    "pipe = initialize_pipeline(vae=vae, unet=unet, text_encoder=text_encoder, tokenizer=tokenizer, noise_scheduler=noise_scheduler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2000473bca35431eac5414048455c5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fd0b1f0a8945429857a800be35421b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9311f250bc4c759b30a8a69cc55dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_before_lora1.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_before_lora2.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_before_lora3.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_lora_replacement(models=[unet, vae, text_encoder], lora_alpha=LORA_ALPHA, lora_rank=LORA_RANK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_all_lora_weights({\"vae\": vae, \"unet\": unet, \"text_encoder\": text_encoder}, 'weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c38a628444439cafde437edc6d3864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6b8d8be0f040f5a68798dbd51e5df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948f85aa08fd4f2d86d4f6f6c25b5db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora1.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora2.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora3.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable LoRA in all models\n",
    "disable_lora(vae)\n",
    "disable_lora(unet)\n",
    "disable_lora(text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5509fc71af7a495caca7e41bf19f1152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3b7399add7468dac8f50caf6fcc60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbdd378acbd4fbfbe37627a5da977ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if LoRA disability is working\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_disable_lora1.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_disable_lora2.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_disable_lora3.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try disable every part of model and check what part of model is crucial for transfer learning using LoRA\n",
    "enable_lora(vae)\n",
    "enable_lora(unet)\n",
    "enable_lora(text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_lora(vae)\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_vae_off1.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_vae_off2.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_vae_off3.png\"))\n",
    "enable_lora(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_lora(unet)\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_unet_off1.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_unet_off2.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_unet_off3.png\"))\n",
    "enable_lora(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_lora(text_encoder)\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_text_encoder_off1.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_text_encoder_off2.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_text_encoder_off3.png\"))\n",
    "enable_lora(text_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It apears that turning off text encoder causes biggest quality loss although text encoder have less parameters than unet.\n",
    "\n",
    "Now try disable LoRA in all model's parts and then activate one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable LoRA in all models\n",
    "disable_lora(vae)\n",
    "disable_lora(unet)\n",
    "disable_lora(text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LoRA only in vae\n",
    "enable_lora(vae)\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora_vae_only1.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora_vae_only2.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora_vae_only3.png\"))\n",
    "disable_lora(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LoRA only in unet\n",
    "enable_lora(unet)\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora_unet_only1.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora_unet_only2.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora_unet_only3.png\"))\n",
    "disable_lora(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LoRA only in text_encoder\n",
    "enable_lora(text_encoder)\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora_text_encoder_only1.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora_text_encoder_only2.png\"))\n",
    "\n",
    "image = pipe(PROMPT).images[0]\n",
    "image.save(os.path.join(ABLATION_STUDY_DIR, PROMPT + \"_lora_text_encoder_only3.png\"))\n",
    "disable_lora(text_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
